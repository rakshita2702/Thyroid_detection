# Install required libraries
!pip install catboost lightgbm xgboost scikit-learn matplotlib seaborn joblib

# Import necessary libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier
import joblib

# Load and preprocess the dataset
def load_and_preprocess_data():
    # Load dataset
    csv_file_path = "hypothyroid.csv"  # Update path if needed
    df = pd.read_csv(csv_file_path)

    # Remove rows containing "?" values
    df.replace("?", np.nan, inplace=True)
    df.dropna(inplace=True)

    # Map binary values
    binary_cols = [
        'on thyroxine', 'query on thyroxine', 'on antithyroid medication', 'sick',
        'pregnant', 'thyroid surgery', 'I131 treatment', 'query hypothyroid',
        'query hyperthyroid', 'lithium', 'goitre', 'tumor', 'hypopituitary',
        'psych', 'TSH measured', 'T3 measured', 'TT4 measured', 'T4U measured',
        'FTI measured', 'TBG measured'
    ]
    for col in binary_cols:
        df[col] = df[col].map({'t': 1, 'f': 0})

    # Encode categorical values
    df['sex'] = df['sex'].map({'M': 1, 'F': 0})
    df['binaryClass'] = df['binaryClass'].map({'P': 1, 'N': 0})
    df['referral source'] = LabelEncoder().fit_transform(df['referral source'])

    # Split features and target
    X = df.drop('binaryClass', axis=1)
    y = df['binaryClass']

    return X, y

# Prepare data
def prepare_data(X, y):
    # Split into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Handle missing values
    imputer = SimpleImputer(strategy='most_frequent')
    X_train = imputer.fit_transform(X_train)
    X_test = imputer.transform(X_test)

    # Standardize the data
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    return X_train, X_test, y_train, y_test

# Train the XGBoost model
def train_xgb_model(X_train, y_train):
    model_xgb = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
    model_xgb.fit(X_train, y_train)
    return model_xgb

# Save the trained model
def save_model(model, file_name='best_model.pkl'):
    joblib.dump(model, file_name)
    print(f"Model saved to {file_name}")

# Main execution for training
if __name__ == "__main__":
    X, y = load_and_preprocess_data()
    X_train, X_test, y_train, y_test = prepare_data(X, y)

    model = train_xgb_model(X_train, y_train)
    save_model(model)
    print(f"Training Accuracy: {accuracy_score(y_train, model.predict(X_train)):.2f}")
    print(f"Test Accuracy: {accuracy_score(y_test, model.predict(X_test)):.2f}")
