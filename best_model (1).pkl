# Install required libraries
!pip install catboost lightgbm xgboost scikit-learn matplotlib seaborn joblib

# Import necessary libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier
import joblib

# Load and preprocess the dataset
def load_and_preprocess_data():
    # Load dataset
    csv_file_path = "hypothyroid.csv"  # Update path if needed
    df = pd.read_csv(csv_file_path)

    # Replace "?" with NaN and drop rows with missing values
    df.replace("?", np.nan, inplace=True)
    df.dropna(inplace=True)

    # Map binary values
    binary_cols = [
        'on thyroxine', 'query on thyroxine', 'on antithyroid medication', 'sick',
        'pregnant', 'thyroid surgery', 'I131 treatment', 'query hypothyroid',
        'query hyperthyroid', 'lithium', 'goitre', 'tumor', 'hypopituitary',
        'psych', 'TSH measured', 'T3 measured', 'TT4 measured', 'T4U measured',
        'FTI measured', 'TBG measured'
    ]
    for col in binary_cols:
        df[col] = df[col].map({'t': 1, 'f': 0})

    # Encode categorical variables
    df['sex'] = df['sex'].map({'M': 1, 'F': 0})
    df['binaryClass'] = df['binaryClass'].map({'P': 1, 'N': 0})
    df['referral source'] = LabelEncoder().fit_transform(df['referral source'])

    # Split features and target
    X = df.drop('binaryClass', axis=1)
    y = df['binaryClass']

    return X, y

# Preprocess data
def prepare_data(X, y):
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Handle missing values
    imputer = SimpleImputer(strategy='most_frequent')
    X_train = imputer.fit_transform(X_train)
    X_test = imputer.transform(X_test)

    # Standardize features
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    return X_train, X_test, y_train, y_test

# Train XGBoost model
def train_xgb_model(X_train, y_train):
    model = XGBClassifier(random_state=42, eval_metric='logloss')
    model.fit(X_train, y_train)
    return model

# Main script to train and save the model
if __name__ == "__main__":
    X, y = load_and_preprocess_data()
    X_train, X_test, y_train, y_test = prepare_data(X, y)
    model = train_xgb_model(X_train, y_train)

    # Save model
    joblib.dump(model, "best_model.pkl")
    print("Model saved as best_model.pkl")
